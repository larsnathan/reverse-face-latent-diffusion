{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Noise schedule and forward diffusion process\n",
    "\n",
    "def cosine_beta_schedule(timesteps):\n",
    "    \"\"\"Generates a noise schedule.\"\"\"\n",
    "    return torch.linspace(1e-4, 0.02, timesteps)\n",
    "\n",
    "def forward_diffusion(x_0, t, noise_schedule):\n",
    "    \"\"\"\n",
    "    Simulates the forward diffusion process by adding noise to the clean image.\n",
    "    x_0: the clean image\n",
    "    t: current timestep\n",
    "    noise_schedule: pre-calculated schedule for the noise\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    return np.sqrt(noise_schedule[t]) * x_0 + np.sqrt(1 - noise_schedule[t]) * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. U-Net model modified to accept embeddings\n",
    "\n",
    "class EmbeddingConditionalUNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, embedding_dim=512):\n",
    "        super(EmbeddingConditionalUNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(c_in, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128 + embedding_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, c_out, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, embedding):\n",
    "        # Encode the image\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Expand and concatenate the embedding with image features\n",
    "        embedding = embedding.unsqueeze(-1).unsqueeze(-1)\n",
    "        embedding = embedding.expand(-1, -1, x.shape[-2], x.shape[-1])\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        \n",
    "        # Process through the bottleneck and decoder\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Training loop with pre-generated embeddings\n",
    "\n",
    "def train_with_custom_embeddings(model, optimizer, timesteps, num_epochs, dataloader, device):\n",
    "    noise_schedule = cosine_beta_schedule(timesteps)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, embeddings in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            embeddings = embeddings.to(device)\n",
    "\n",
    "            # Sample a random timestep\n",
    "            t = torch.randint(0, timesteps, (1,)).long().to(device)\n",
    "\n",
    "            # Apply forward diffusion to get a noisy image\n",
    "            noisy_images = forward_diffusion(images, t, noise_schedule)\n",
    "\n",
    "            # Pass the noisy image and the pre-generated embeddings to the model\n",
    "            noise_pred = model(noisy_images, embeddings)\n",
    "\n",
    "            # Compute the loss and update the model\n",
    "            loss = mse_loss(noise_pred, noisy_images)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Sampling process to generate images from embeddings\n",
    "\n",
    "def sample_with_embeddings(model, embeddings, timesteps, device):\n",
    "    noise_schedule = cosine_beta_schedule(timesteps)\n",
    "    with torch.no_grad():\n",
    "        # Start from pure noise\n",
    "        x_t = torch.randn(1, 3, 64, 64).to(device)\n",
    "\n",
    "        for t in reversed(range(timesteps)):\n",
    "            # Predict the noise, conditioned on the embeddings\n",
    "            noise_pred = model(x_t, embeddings)\n",
    "\n",
    "            # Reverse the diffusion process step-by-step\n",
    "            x_t = (x_t - np.sqrt(1 - noise_schedule[t]) * noise_pred) / np.sqrt(noise_schedule[t])\n",
    "\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1001/202599 [00:00<01:04, 3113.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1. Define a custom dataset that loads both images and embeddings\n",
    "class ImageEmbeddingDataset(Dataset):\n",
    "    def __init__(self, image_folder, embedding_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.embedding_folder = embedding_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # List all image files (.jpg) and only keep ones with corresponding .npy files\n",
    "        self.image_files = []\n",
    "        self.embedding_files = []\n",
    "        \n",
    "        # Loop through all image files and check if the corresponding embedding exists\n",
    "        for image_file in tqdm(os.listdir(image_folder)):\n",
    "            if image_file.endswith('.jpg'):\n",
    "                embedding_file = image_file.replace('.jpg', '.npy')\n",
    "                if os.path.exists(os.path.join(embedding_folder, embedding_file)):\n",
    "                    self.image_files.append(image_file)\n",
    "                    self.embedding_files.append(embedding_file)\n",
    "            if len(self.image_files) > 1000:\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image and embedding filenames\n",
    "        image_file = self.image_files[idx]\n",
    "        embedding_file = self.embedding_files[idx]\n",
    "        \n",
    "        # Load the image\n",
    "        image_path = os.path.join(self.image_folder, image_file)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load the embedding (as a numpy array)\n",
    "        embedding_path = os.path.join(self.embedding_folder, embedding_file)\n",
    "        embedding = np.load(embedding_path)\n",
    "\n",
    "        # Convert embedding to a tensor\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "        return image, embedding\n",
    "\n",
    "# 2. Define the image transformations (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64 if needed\n",
    "    transforms.ToTensor(),        # Convert PIL image to Tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# 3. Create the dataset\n",
    "image_folder = 'data/img_align_celeba'\n",
    "embedding_folder = 'data/embeddings'\n",
    "\n",
    "dataset = ImageEmbeddingDataset(image_folder, embedding_folder, transform=transform)\n",
    "\n",
    "# 4. Create the DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now you can use the dataloader in your training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsond1\\AppData\\Local\\Temp\\ipykernel_58164\\2027171523.py:15: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  return np.sqrt(noise_schedule[t]) * x_0 + np.sqrt(1 - noise_schedule[t]) * noise\n",
      "100%|██████████| 32/32 [02:05<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.3010968565940857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:49<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.1938410997390747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:47<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.18654359877109528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:38<01:24,  3.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Assume you have a dataloader that yields (images, embeddings)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# For example: dataloader = [(image_batch, embedding_batch), ...]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain_with_custom_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Sample from the model using a specific embedding\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# sampled_image = sample_with_embeddings(model, embedding_batch[0], timesteps, device)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Save or display the generated image\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# save_image(sampled_image, 'generated_image.png')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mtrain_with_custom_embeddings\u001b[1;34m(model, optimizer, timesteps, num_epochs, dataloader, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mse_loss(noise_pred, noisy_images)\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\larsond1\\Anaconda3\\envs\\aaml_paper\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larsond1\\Anaconda3\\envs\\aaml_paper\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larsond1\\Anaconda3\\envs\\aaml_paper\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 5. Example usage\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 1000\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, optimizer, and dataset\n",
    "embedding_dim = 512  # Adjust this to the size of your embeddings\n",
    "model = EmbeddingConditionalUNet(embedding_dim=embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Assume you have a dataloader that yields (images, embeddings)\n",
    "# For example: dataloader = [(image_batch, embedding_batch), ...]\n",
    "\n",
    "# Train the model\n",
    "train_with_custom_embeddings(model, optimizer, timesteps, num_epochs, dataloader, device)\n",
    "\n",
    "# Sample from the model using a specific embedding\n",
    "# sampled_image = sample_with_embeddings(model, embedding_batch[0], timesteps, device)\n",
    "\n",
    "# Save or display the generated image\n",
    "# save_image(sampled_image, 'generated_image.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaml_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
